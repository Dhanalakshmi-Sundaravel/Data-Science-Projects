{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample code for Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load iris and split it into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris_X_train, iris_X_test, iris_y_train, iris_y_test = train_test_split(iris['data'], iris['target'], random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the sizes of the training and test sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now load the ionosphere dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "is_data = np.genfromtxt(\"ionosphere.txt\", delimiter=\",\", usecols=np.arange(34))\n",
    "is_target = np.genfromtxt(\"ionosphere.txt\", delimiter=\",\", usecols=34, dtype='int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting it into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_X_train, is_X_test, is_y_train, is_y_test = train_test_split(is_data, is_target, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the sizes of the training and test sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(263, 34)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, 34)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we implement the basic Nearest Neighbour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbour for iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let us write a simple function for computing Euclidean distance and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4142135623730951"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dist(x1,x2):\n",
    "  return np.linalg.norm(x1-x2)\n",
    "dist(np.array([0,0]),np.array([1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative to using the function np.linalg.norm, computing the Euclidean norm of a vector, is the function you wrote for Exercise 6 in Section 7 of Lab Worksheet 3.  (That function computes the squared Euclidean norm, but this will not affect the output of the Nearest Neighbour algorithm or the conformal predictor based on it.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function finds the nearest neighbour to x in X. We also have a very primitive test of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.4142135623730951, 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "def nearest(x,X):\n",
    "  current_record = math.inf\n",
    "  for n in range(X.shape[0]):\n",
    "    current_dist = dist(x,X[n])\n",
    "    if current_dist < current_record:\n",
    "      current_record = current_dist\n",
    "      current_record_holder = n\n",
    "  return current_record, current_record_holder\n",
    "nearest(np.array([1,1,0]),np.zeros((3,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go over the test set applying the function \"nearest\" to each test sample.  Along the way we compute the number ot errors and print all errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 37\n",
      "Number of errors: 1\n",
      "Error rate: 0.02631578947368421\n",
      "0.03846240043640137 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "n_iris_test = iris_X_test.shape[0]\n",
    "n_errors = 0\n",
    "prediction = np.zeros(n_iris_test,dtype=int)\n",
    "for n in range(n_iris_test):\n",
    "  prediction[n] = iris_y_train[nearest(iris_X_test[n],iris_X_train)[1]]\n",
    "  if prediction[n] != iris_y_test[n]:\n",
    "    n_errors = n_errors + 1\n",
    "    print(\"Error:\",n)\n",
    "print(\"Number of errors:\",n_errors)\n",
    "print(\"Error rate:\",n_errors / n_iris_test)\n",
    "print(time.time() - start,\"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see our predictions and the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 0, 2, 1,\n",
       "       0, 0, 2, 0, 0, 1, 1, 0, 2, 1, 0, 2, 2, 1, 0, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 0, 2, 1,\n",
       "       0, 0, 2, 0, 0, 1, 1, 0, 2, 1, 0, 2, 2, 1, 0, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbour for ionosphere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program for ionospehere is very similar, but now we do not bother remembering the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 2\n",
      "Error: 6\n",
      "Error: 9\n",
      "Error: 34\n",
      "Error: 42\n",
      "Error: 47\n",
      "Error: 50\n",
      "Error: 54\n",
      "Error: 55\n",
      "Error: 69\n",
      "Error: 75\n",
      "Error: 83\n",
      "Error: 85\n",
      "Number of errors: 13\n",
      "Error rate: 0.14772727272727273\n",
      "0.20167231559753418 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "n_is_test = is_X_test.shape[0]\n",
    "n_errors = 0\n",
    "for n in range(n_is_test):\n",
    "  prediction = is_y_train[nearest(is_X_test[n],is_X_train)[1]]\n",
    "  if prediction != is_y_test[n]:\n",
    "    print(\"Error:\",n)\n",
    "    n_errors = n_errors + 1\n",
    "print(\"Number of errors:\",n_errors)\n",
    "print(\"Error rate:\",n_errors / n_is_test)\n",
    "print(time.time() - start,\"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for random state 0 are: 97% accuracy on iris and 85% accuracy for ionosphere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conformal predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will implement conformal predictors in the conceptually cleanest way following their definition.  They are, however, computationally inefficient and take a few minutes to run on my laptop.  If you would like to experiment with conformal predictors, you will be better off moving to the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conformal predictor for iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have a function computing the confomity score of the kth element of a bag (X,y) (where X is the data and y is the labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conformity(X,y,k):   # test object: the kth row\n",
    "  current_record_own = current_record_other = math.inf\n",
    "  for n in range(X.shape[0]):\n",
    "    current_dist = dist(X[k],X[n])\n",
    "    if (current_dist < current_record_own) and (y[n]==y[k]) and (n!=k):\n",
    "      current_record_own = current_dist\n",
    "    if (current_dist < current_record_other) and (y[n]!=y[k]) and (n!=k):\n",
    "      current_record_other = current_dist\n",
    "  if current_record_own == 0:\n",
    "    if (current_record_other == 0):\n",
    "      print(\"Division 0/0\")    # sanity check\n",
    "      return 0                 # see below\n",
    "    return math.inf\n",
    "  else:\n",
    "    return current_record_other / current_record_own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Division 0/0\n",
      "Division 0/0\n",
      "Division 0/0\n",
      "Division 0/0\n",
      "Error:  37\n",
      "Number of errors:  1\n",
      "Mean false p-value:  0.011294829995342348\n"
     ]
    }
   ],
   "source": [
    "n_iris_train = iris_X_train.shape[0]\n",
    "n_iris_test = iris_X_test.shape[0]\n",
    "n_errors = 0\n",
    "sum_p_values = 0.0\n",
    "prediction = np.zeros(n_iris_test,dtype=int)\n",
    "score = np.zeros(n_iris_train+1)  # the conformity scores\n",
    "p = np.zeros(3)    # the p-values\n",
    "for n in range(n_iris_test):\n",
    "  augmented_X = np.row_stack((iris_X_train,iris_X_test[n]))\n",
    "  for l in range(3):   # postulated label\n",
    "    augmented_y = np.append(iris_y_train,l)\n",
    "    for m in range(n_iris_train+1):\n",
    "      score[m] = conformity(augmented_X,augmented_y,m)\n",
    "    p[l] = np.mean(score<=score[n_iris_train])  # the p-value of l\n",
    "    # the formula in the lectures: p[l] = sum(score<=score[n_iris_train]) / (n_iris_train+1)\n",
    "  prediction[n] = np.argmax(p)   # the smallest argmax\n",
    "  if prediction[n] != iris_y_test[n]:\n",
    "    n_errors = n_errors + 1\n",
    "    print(\"Error: \",n)\n",
    "  sum_p_values = sum_p_values + p[0] + p[1] + p[2] - p[iris_y_test[n]]\n",
    "print(\"Number of errors: \", n_errors)\n",
    "print(\"Mean false p-value: \", sum_p_values/(2*n_iris_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am getting four instances of \"Division 0/0\" for random state 0.  Is it really true that there are identical samples that belong to different classes in the iris dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples  9  and  34  are identical, with labels  0  and  0\n",
      "Samples  9  and  37  are identical, with labels  0  and  0\n",
      "Samples  34  and  37  are identical, with labels  0  and  0\n",
      "Samples  101  and  142  are identical, with labels  2  and  2\n"
     ]
    }
   ],
   "source": [
    "n_iris = iris['data'].shape[0]\n",
    "for i in range(n_iris-1):\n",
    "  for j in range(i+1,n_iris):\n",
    "    current_dist = dist(iris['data'][i],iris['data'][j])\n",
    "    if (current_dist == 0): # and ():\n",
    "      print(\"Samples \", i, \" and \",j,\" are identical, with labels \",iris['target'][i], \" and \", iris['target'][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no identical samples with different labels!  What do you think went wrong?  And why do you think I defined 0/0 to be 0 in the function \"conformity\"?  (By the way, it's a standard convention.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conformal predictor for ionosphere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following program is very similar, but there is an important source of complications: the labels are {-1,1} rather than {0,1}.  We still code the labels by {0,1}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:  2\n",
      "Error:  6\n",
      "Error:  9\n",
      "Error:  34\n",
      "Error:  42\n",
      "Error:  47\n",
      "Error:  50\n",
      "Error:  54\n",
      "Error:  55\n",
      "Error:  69\n",
      "Error:  75\n",
      "Error:  83\n",
      "Error:  85\n",
      "Number of errors:  13\n",
      "Error rate:  0.14772727272727273\n",
      "Mean false p-value:  0.06099345730027552\n"
     ]
    }
   ],
   "source": [
    "n_is_train = is_X_train.shape[0]\n",
    "n_is_test = is_X_test.shape[0]\n",
    "n_errors = 0\n",
    "sum_p_values = 0.0\n",
    "prediction = np.zeros(n_is_test,dtype=int)\n",
    "score = np.zeros(n_is_train+1)  # the conformity scores\n",
    "p = np.zeros(2)    # the p-values\n",
    "for n in range(n_is_test):\n",
    "  augmented_X = np.vstack((is_X_train,is_X_test[n]))\n",
    "  for l in range(2):   # code of the postulated label, 0 standing for -1 and 1 for 1\n",
    "    augmented_y = np.append(is_y_train,2*l-1)  # 2*l-1 is the postulated label\n",
    "    for m in range(n_is_train+1):\n",
    "      score[m] = conformity(augmented_X,augmented_y,m)\n",
    "    p[l] = np.mean(score<=score[n_is_train])  # the p-value of l\n",
    "    # p[l] = sum(score<=score[n_is_train]) / (n_is_train+1)\n",
    "  prediction[n] = 2*np.argmax(p)-1   # the smallest argmax, and the code is transformed into the label\n",
    "  if prediction[n] != is_y_test[n]:\n",
    "    n_errors = n_errors + 1\n",
    "    print(\"Error: \",n)\n",
    "  sum_p_values = sum_p_values + p[0] + p[1] - p[(is_y_test[n]+1)//2]  # in p[...]: turning label into code\n",
    "print(\"Number of errors: \", n_errors)\n",
    "print(\"Error rate: \",n_errors / n_is_test)\n",
    "print(\"Mean false p-value: \", sum_p_values/n_is_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief discussion of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting that the simple Nearest Neighbour and conformalized Nearest Neighbour make the same numbers of errors and, moreover, they make errors on the same test samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More efficient conformal predictors (with preprocessing the training set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conformal predictors of the previous section are extremely inefficient; we kept performing the same calculations over and over again.  We make them more efficient by preprocessing the training set.  Exercise: think of further ways of improving efficiency.  Ideally, the number of calls to dist should be n(n-1)/2+kn, where n is the size of the training set and k is the size of the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more efficient conformal predictor for iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we preprocess the training set by computing the distance of each training sample to its nearest neghbour in the training set of the same and other class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following two commands are only needed if you skipped the previous section.\n",
    "# (But they are harmless.)\n",
    "n_iris_train = iris_X_train.shape[0]\n",
    "n_iris_test = iris_X_test.shape[0]\n",
    "dist_own = math.inf * np.ones(n_iris_train)  # shortest distances to the own class, initialized to infinity\n",
    "dist_other = math.inf * np.ones(n_iris_train)  # shortest distances to the other class, initialized to infinity\n",
    "for i in range(n_iris_train-1):\n",
    "  for j in range(i+1,n_iris_train):\n",
    "    current_dist = dist(iris_X_train[i],iris_X_train[j])\n",
    "    if iris_y_train[i]==iris_y_train[j]:\n",
    "      if (current_dist < dist_own[i]):\n",
    "        dist_own[i] = current_dist\n",
    "      if (current_dist < dist_own[j]):\n",
    "        dist_own[j] = current_dist\n",
    "    else:\n",
    "      if (current_dist < dist_other[i]):\n",
    "        dist_other[i] = current_dist\n",
    "      if (current_dist < dist_other[j]):\n",
    "        dist_other[j] = current_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to process the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Division 0/0\n",
      "Division 0/0\n",
      "Division 0/0\n",
      "Division 0/0\n",
      "Error:  37\n",
      "Number of errors:  1\n",
      "Mean false p-value:  0.011294829995342348\n"
     ]
    }
   ],
   "source": [
    "n_errors = 0\n",
    "sum_p_values = 0.0\n",
    "prediction = np.zeros(n_iris_test,dtype=int)\n",
    "score = np.zeros(n_iris_train+1)  # the conformity scores\n",
    "p = np.zeros(3)    # the p-values\n",
    "\n",
    "for j in range(n_iris_test):\n",
    "  for l in range(3):   # postulated label\n",
    "    aug_dist_own = np.append(dist_own,math.inf)\n",
    "    aug_dist_other = np.append(dist_other,math.inf)\n",
    "    for i in range(n_iris_train):\n",
    "      current_dist = dist(iris_X_train[i],iris_X_test[j])\n",
    "      if iris_y_train[i]==l:\n",
    "        if (current_dist < aug_dist_own[i]):\n",
    "          aug_dist_own[i] = current_dist\n",
    "        if (current_dist < aug_dist_own[n_iris_train]):\n",
    "          aug_dist_own[n_iris_train] = current_dist\n",
    "      else:\n",
    "        if (current_dist < aug_dist_other[i]):\n",
    "          aug_dist_other[i] = current_dist\n",
    "        if (current_dist < aug_dist_other[n_iris_train]):\n",
    "          aug_dist_other[n_iris_train] = current_dist\n",
    "    # the following for loop is the careful version of score = aug_dist_other / aug_distance_own\n",
    "    for i in range(n_iris_train+1):\n",
    "      if aug_dist_own[i] == 0:\n",
    "        score[i] = math.inf\n",
    "        if (aug_dist_other[i] == 0):\n",
    "          print(\"Division 0/0\")    # sanity check\n",
    "          score[i] = 0\n",
    "      else:\n",
    "        score[i] = aug_dist_other[i] / aug_dist_own[i]\n",
    "    p[l] = np.mean(score<=score[n_iris_train])  # the p-value of l\n",
    "    # p[l] = sum(score<=score[n_iris_train]) / (n_iris_train+1)  # as in Chapter 3\n",
    "  prediction[j] = np.argmax(p)   # the smallest argmax\n",
    "  if prediction[j] != iris_y_test[j]:\n",
    "    n_errors = n_errors + 1\n",
    "    print(\"Error: \",j)\n",
    "  sum_p_values = sum_p_values + p[0] + p[1] + p[2] - p[iris_y_test[j]]\n",
    "print(\"Number of errors: \", n_errors)\n",
    "print(\"Mean false p-value: \", sum_p_values/(2*n_iris_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more efficient conformal predictor for ionosphere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program for ionosphere is very similar, but with the same complication as in the previous section.  First we preprocess the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following two commands are only needed if you skipped the previous section.\n",
    "n_is_train = is_X_train.shape[0]\n",
    "n_is_test = is_X_test.shape[0]\n",
    "dist_own = math.inf * np.ones(n_is_train)  # shortest distances to the own class, initialized to infinity\n",
    "dist_other = math.inf * np.ones(n_is_train)  # shortest distances to the own class, initialized to infinity\n",
    "for i in range(n_is_train-1):\n",
    "  for j in range(i+1,n_is_train):\n",
    "    current_dist = dist(is_X_train[i],is_X_train[j])\n",
    "    if is_y_train[i]==is_y_train[j]:\n",
    "      if (current_dist < dist_own[i]):\n",
    "        dist_own[i] = current_dist\n",
    "      if (current_dist < dist_own[j]):\n",
    "        dist_own[j] = current_dist\n",
    "    else:\n",
    "      if (current_dist < dist_other[i]):\n",
    "        dist_other[i] = current_dist\n",
    "      if (current_dist < dist_other[j]):\n",
    "        dist_other[j] = current_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can process the test set more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:  2\n",
      "Error:  6\n",
      "Error:  9\n",
      "Error:  34\n",
      "Error:  42\n",
      "Error:  47\n",
      "Error:  50\n",
      "Error:  54\n",
      "Error:  55\n",
      "Error:  69\n",
      "Error:  75\n",
      "Error:  83\n",
      "Error:  85\n",
      "Number of errors:  13\n",
      "Mean false p-value:  0.06099345730027552\n"
     ]
    }
   ],
   "source": [
    "n_errors = 0\n",
    "sum_p_values = 0.0\n",
    "prediction = np.zeros(n_is_test,dtype=int)\n",
    "score = np.zeros(n_is_train+1)  # the conformity scores\n",
    "p = np.zeros(2)    # the p-values\n",
    "\n",
    "for j in range(n_is_test):\n",
    "  for l in range(2):   # postulated label code\n",
    "    aug_dist_own = np.append(dist_own,math.inf)\n",
    "    aug_dist_other = np.append(dist_other,math.inf)\n",
    "    for i in range(n_is_train):\n",
    "      current_dist = dist(is_X_train[i],is_X_test[j])\n",
    "      if is_y_train[i]==2*l-1:\n",
    "        if (current_dist < aug_dist_own[i]):\n",
    "          aug_dist_own[i] = current_dist\n",
    "        if (current_dist < aug_dist_own[n_is_train]):\n",
    "          aug_dist_own[n_is_train] = current_dist\n",
    "      else:\n",
    "        if (current_dist < aug_dist_other[i]):\n",
    "          aug_dist_other[i] = current_dist\n",
    "        if (current_dist < aug_dist_other[n_is_train]):\n",
    "          aug_dist_other[n_is_train] = current_dist\n",
    "    # the following for loop is the careful version of score = aug_dist_other / aug_distance_own\n",
    "    for i in range(n_is_train+1):\n",
    "      if aug_dist_own[i] == 0:\n",
    "        score[i] = math.inf\n",
    "        if (aug_dist_other[i] == 0):\n",
    "          print(\"Division 0/0\")    # sanity check\n",
    "          score[i] = 0\n",
    "      else:\n",
    "        score[i] = aug_dist_other[i] / aug_dist_own[i]\n",
    "    p[l] = np.mean(score<=score[n_is_train])  # the p-value of l\n",
    "    # p[l] = sum(score<=score[n_is_train]) / (n_is_train+1)\n",
    "  prediction[j] = 2*np.argmax(p)-1   # the smallest argmax, turned into a label\n",
    "  if prediction[j] != is_y_test[j]:\n",
    "    n_errors = n_errors + 1\n",
    "    print(\"Error: \",j)\n",
    "  sum_p_values = sum_p_values + p[0] + p[1] - p[(is_y_test[j]+1)//2]\n",
    "print(\"Number of errors: \", n_errors)\n",
    "print(\"Mean false p-value: \", sum_p_values/n_is_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark about the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the results we are getting in this section are identical to those in the previous section (we have just removed superfluous computations)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
